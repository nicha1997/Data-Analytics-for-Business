{
  "metadata": {
    "kernelspec": {
      "display_name": "Pyolite",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicha1997/Data-Analytics-for-Business/blob/main/LSTM_Model_for_2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import layers, Input\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import data\n",
        "file_path = r'D:\\desktop\\set.xlsx'  # Local file path\n",
        "data = pd.read_excel(file_path)\n",
        "dates = pd.to_datetime(data['date'])  # Convert the date to date format\n",
        "\n",
        "# Define a function to create a dataset, considering leap years.\n",
        "def create_dataset_for_single_item(dates, dataset, lookback_years=4):\n",
        "    dataX, dataY = [], []\n",
        "    target_dates = pd.date_range(start='2017-01-01', end='2017-12-31')  # Target dates are the dates from 2017.\n",
        "\n",
        "    for target_date in target_dates:\n",
        "        x = []\n",
        "        # Get data from the past 4 years.\n",
        "        for year in range(lookback_years):\n",
        "            current_date = target_date - pd.DateOffset(years=year)  # Looking back at past years.\n",
        "\n",
        "            # Find the index of the current date in the data.\n",
        "            current_date_idx = np.where(dates == current_date)[0]\n",
        "\n",
        "            if len(current_date_idx) > 0:\n",
        "                x.append(dataset[current_date_idx[0]])\n",
        "            else:\n",
        "                print(f\"Date not found: {Current_date} in dataset\")  # Debug information.\n",
        "\n",
        "        if len(x) == lookback_years:  # Ensure there is complete data for four years.\n",
        "            dataX.append(x)\n",
        "            # The sales on the current date in 2017 as the label.\n",
        "            target_idx = np.where(dates == target_date)[0]\n",
        "            if len(target_idx) > 0:\n",
        "                dataY.append(dataset[target_idx[0]])\n",
        "            else:\n",
        "                print(f\"Target date not found: {target_date} in dataset\")  # Debug information.\n",
        "\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# Predict 'store1item1'.\n",
        "target_column = 'store1item1'\n",
        "print(f\"Predicting sales of {target_comlumn}...\")\n",
        "\n",
        "# Get the sales data for 'store1item1'.\n",
        "dataset = data[target_column].values\n",
        "\n",
        "# Normalize the data.\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_target = scaler.fit_transform(dataset.reshape(-1, 1))  # Normalize the target data.\n",
        "\n",
        "# Call the function to create the dataset.\n",
        "trainX, trainY = create_dataset_for_single_item(dates, scaled_target, lookback_years=4)\n",
        "\n",
        "# Debug information.\n",
        "print(f\"trainX shape: {trainX.shape}, trainY shape: {trainY.shape}\")\n",
        "\n",
        "# Check if enough data has been generated.\n",
        "if len(trainX) == 0 or len(trainY) == 0:\n",
        "    print(\"The generated dataset is empty, please check the input data and date range.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "split_index = int(0.8 * len(trainX))\n",
        "X_train, X_test = trainX[:split_index], trainX[split_index:]\n",
        "y_train, y_test = trainY[:split_index], trainY[split_index:]\n",
        "\n",
        "# Ensure the dimensions of X_train and X_test are correct.\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the LSTM model.\n",
        "input_shape = Input(shape=(X_train.shape[1], 1))  # Four years of daily data.\n",
        "lstm1 = layers.LSTM(128, return_sequences=True)(input_shape)\n",
        "lstm2 = layers.LSTM(256)(lstm1)\n",
        "dense1 = layers.Dense(128, activation=\"relu\")(lstm2)\n",
        "dropout = layers.Dropout(rate=0.2)(dense1)\n",
        "output_shape = layers.Dense(1)(dropout)\n",
        "\n",
        "# Define the model.\n",
        "lstm_model = tf.keras.Model(inputs=input_shape, outputs=output_shape)\n",
        "lstm_model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"mse\"])\n",
        "\n",
        "# Train the model.\n",
        "lstm_model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Predict the testing set.\n",
        "y_pred = lstm_model.predict(X_test)\n",
        "\n",
        "# Inverse normalize the predicted and actual values.\n",
        "y_test_real = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "y_pred_real = scaler.inverse_transform(y_pred)\n",
        "\n",
        "# Calculate RMSE and R².\n",
        "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
        "r2 = r2_score(y_test_real, y_pred_real)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R²: {r2}\")\n",
        "\n",
        "# Save the actual and predicted values as a CSV file.\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Date': pd.date_range(start='2017-01-01', periods=len(y_test_real)),\n",
        "    'Actual': y_test_real.flatten(),\n",
        "    'Predicted': y_pred_real.flatten(),\n",
        "    'StoreItem': target_column\n",
        "})\n",
        "\n",
        "output_file_path = r'D:\\desktop\\store1item1_actual_vs_predicted_2017.csv'\n",
        "comparison_df.to_csv(output_file_path, index=False)\n",
        "print(f\"The actual and predicted values have been saved as CSV files:{output_file_path}\")\n",
        "\n",
        "# Plot a comparison chart of actual values and predicted values.\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(comparison_df['Date'], comparison_df['Actual'], label='Actual', color='blue')\n",
        "plt.plot(comparison_df['Date'], comparison_df['Predicted'], label='Predicted', color='red')\n",
        "plt.title(f'Actual vs Predicted for {target_column} in 2017')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mOkj8ABDGDXN"
      },
      "id": "mOkj8ABDGDXN",
      "execution_count": null,
      "outputs": []
    }
  ]
}